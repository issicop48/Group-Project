{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90b962f0-518d-462b-9405-63e3a133f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  do you have a newsletter \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, we do provide a daily newspaper in the lobby for our guests."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import pinecone\n",
    "from pinecone import Pinecone\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "#venu's pinecone\n",
    "pc = Pinecone(api_key=\"84430aa5-b475-4aad-b581-37d56c9f71de\")\n",
    "index_name = pc.Index(\"project\")\n",
    "\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "hf_token = \"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\"\n",
    "api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "def generate_embedding(text):\n",
    "    response = requests.post(api_url, headers=headers, json={\"inputs\": text, \"options\": {\"wait_for_model\": True}})\n",
    "    embedding = response.json()  # Directly get the list of embeddings\n",
    "    # Limit the embedding to the first 300 dimensions if the dimension is larger than 300\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Asks the user for a question\"\"\"\n",
    "    prompt = input(\"Enter your query: \")\n",
    "    return prompt\n",
    "\n",
    "def retrieval_augmented_generation(query):\n",
    "    \"\"\"Gets top 3 matches from Pinecone and prepares prompt for LLaMA\"\"\"\n",
    "    start_time = time.time()\n",
    "    max_timeout = 1000  # Timeout in seconds\n",
    "\n",
    "    # Retrieve relevant documents from Pinecone\n",
    "    embedding = generate_embedding(query)\n",
    "    results = index_name.query(vector=embedding, top_k=3, include_metadata=True)\n",
    "\n",
    "    retrieved_info = [item['metadata'] for item in results['matches']]\n",
    "    # Prepare augmented prompt\n",
    "    augmented_prompt = query\n",
    "    for item in retrieved_info:\n",
    "        augmented_prompt = \"you are a hotel assistant, your job is to help answer any questions from the hotel customers. Here is a question from one of our customers\" + augmented_prompt + \" Here are some previous frequently asked answer and questions that you can use to answer their question \" + str(item) + \"if you do not find relevant context from the above messge, use your best judgement to answer the question. only respond back with one answer, do not add any additional text to your answer as this response is relayed direclty to the customer. keep it short and elegant so the customer feels nice\"\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > max_timeout:\n",
    "        return \"Response generation timed out after 100 seconds.\"\n",
    "    else:\n",
    "        return augmented_prompt  \n",
    "\n",
    "def generate_response(augmented_prompt):\n",
    "    \"\"\"Generates a response using LLaMA\"\"\"\n",
    "    stream = ollama.chat(\n",
    "    model='llama2',\n",
    "    messages=[{'role': 'user', 'content': augmented_prompt}],\n",
    "    stream=True,\n",
    "    )\n",
    "    # print(\"test\")   #debug\n",
    "    # print(augmented_prompt)   #debug\n",
    "    # print(\"test\")   #debug\n",
    "    for chunk in stream:\n",
    "      print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = get_user_input()\n",
    "    augmented_prompt = retrieval_augmented_generation(query)\n",
    "    response = generate_response(augmented_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06702f4-2758-4e31-b6e9-5503c0a07927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
